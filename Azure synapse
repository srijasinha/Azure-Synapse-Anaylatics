Azure Synapse Anlaytics will bring both Azure Datawarehouse and Big Data Warehousing together.
It is a single service ,combination of ADF, ADLS(Azure data lake storage),ADB - Azure DataBicks, SQL DB  AND File system in ADLS Gen2
Fundamentals Topics:
1)Azure  Synapse Workspace: we can perform data warehouse activity,ETL operations,all cloud based activity, data engineering., Data Transformation Logics.
It contains  File system and ADLS Gen2. We can run SQL quruies directly of run Notebooks directly throgh SQL pool.

2)Linked Service: Its a connection string that connect the workspace with external resources.

3)Synapse SQL -It is abality to do T- SQL based anaylitics in synapse workspace.Synapse SQL pool helps to run sql scripts.
Sql Pools has two types of consumption  models- 
1)serverless - It uses to run sql queries automitically /(Built in feature).Workspace will use for the physical resources for a time only when we run the query.
2)Dedicated- We are keeping the physical resources always available even we are not running the query.We can crete a new Dedicated SQL Pool. 

4)Apache synapse for sparks:- We can run spark jobs.We can create apache spark pools in a workspace.
Two ways to use spark in synapse- 
1)Spark Notebook-For using data science and data enginner using scale, spark SQL,c#
2)Spark Job Notification-For running batch spark jobs using jar files.

5)Pipelines- Pipeline are logical group of activities that perform task together. 
a)Trigger
b)Dataset
c)Data Flows

Praactical:-
We need to create new Azure Synapse Anlaytics workspace.It will ask Data Lake storage Gen2
Azure Data Lake Storage Gen2 (ADLS Gen2) - is a cloud-based data storage service built on Azure Blob Storage, designed for big data analytics.
We use Azure synapse studio

1)How to anylase data in serverless SQL pool in Azure synapse Anlytics
Ans- Let you use SQL without having to reserve capacity.Every workspace comes with a pre configured serverless  SQL pool cslled Built in.
solution- upload he dat in linked service->In develop(create a sql script)(connected to Buit in)
OpenRowSet() function allows you to access files in azure storage and return the content as set of rows.
This Data can be visualized by switching Table to chart view.

2)Dedicated SQL Pool -It conumes billable resources if it is active.You can pause the pool to reduce the cost.
Dedicated SQl pool is associated with dedicated SQL DB.
solution- create a new Dedicated SQL Pool ->upload he dat in linked service->In develop(create a sql script)(connected to Dedicated SQL Pool)

3)How to anylase data in serverless Spark pool in Azure synapse Anlytics-
Spark is a technology used for BIGData analytics
Inside spark we have clusters- Group of virtual machines.These VM will be called as nodes in case of spark.
Based on Data spark pool will decide how many nodes should be get used
In synapse we will crease a spark pool. ->Notebook will have logic.
Solution:-Create a spark Pool->In develop(create a Notebook)(connected to Spark Pool)

4)Analyze data in storage accont in Azure synapse Anylatics
a)create a csv file and parquet files in storage account 
b)Analyze data from thaat files
solution- crate a notebook ->write code in pyspark to generate csv file -  df= sparrk.sql("select * from tablename")
Attach the notebook to serveless Spark pool

5) How to Integrate Pipeline in ASA (Azure synapse Anylatics)
solution- crate a new pipeline->Notebook->make changes in settings(select notebook which you want to run)->Run this pipeline-<Shedule this pipeline

6)Monitor Pipelines,trigger runs,sql activities ,apache spark activitiess
solution- Got to Monitor tab and check all details

7)How to add an administrator to ASA?
1)RBASC- Rose Based Aces control- (Azure workspace Role based acess)
Azure portal->Acess control(IAM)- under this section we can add RBAC->Add roll assignment
2)Synapse based RBASC- Synapse administration role for workspace
Open Azure synapse Anylatics->Managment tab->Acess control->Add_>workspace->synapse Administrator->user name
3)Axure RBAC-- Below role assignments on the workspace's primary storage
Azure portal->primary storage portal->Acess control(IAM)-add roles->owner and ->Storage Blob data contributer
4)Dedicated SQL Pool
Add db_owner role
